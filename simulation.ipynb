{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Define a bias value X\n",
    "X = 0.5 # TODO: Change this value to the desired bias value\n",
    "L=\"ol\" # TODO: Change this value to 'hs' or 'ol' to select the desired label\n",
    "\n",
    "if L == \"hs\":\n",
    "    LL = 'hate.speech'\n",
    "elif L == \"ol\":\n",
    "    LL = 'offensive.language'\n",
    "\n",
    "# read the csvs from emnlp paper\n",
    "train = pd.read_csv('data/full_test_s.csv')\n",
    "test = pd.read_csv('data/full_train_s.csv')\n",
    "# Combine the training and testing datasets into one DataFrame\n",
    "all_data = pd.concat([train, test], axis=0)\n",
    "\n",
    "# Sort the combined dataset by 'tweet.id' and 'version' in ascending order\n",
    "all_data.sort_values(by=['tweet.id', 'version'], ascending=[True, True], inplace=True)\n",
    "\n",
    "# Calculate the average hate speech label (p_i) for each 'tweet.id' across all annotators and versions (15 versions)\n",
    "all_data['p_i'] = all_data.groupby('tweet.id')[LL].transform('mean')\n",
    "\n",
    "# Compute biased probabilities p^A_i and p^B_i for each tweet by adjusting p_i by X\n",
    "all_data['p^A_i'] = all_data['p_i'] - X  # Bias towards reducing hate speech probability\n",
    "all_data['p^B_i'] = all_data['p_i'] + X  # Bias towards increasing hate speech probability\n",
    "\n",
    "# Ensure that the biased probabilities are within the valid range [0, 1]\n",
    "all_data['p^A_i'] = all_data['p^A_i'].clip(0, 1)\n",
    "all_data['p^B_i'] = all_data['p^B_i'].clip(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split: 500 for test\n",
    "df_train = all_data[(all_data['tweet.id'] >= 1) & (all_data['tweet.id'] <= 2500)]\n",
    "df_test = all_data[(all_data['tweet.id'] >= 2501) & (all_data['tweet.id'] <= 3000)]\n",
    "\n",
    "df_test.to_csv(f\"data/test_{L}_label_{X}.csv\")\n",
    "\n",
    "## Balanced dataset (2 A, 2 B)\n",
    "# Create a balanced dataset with equal representation from annotator types A and B\n",
    "df_balanced = df_train\n",
    "df_balanced.to_csv(f\"data/balanced_{L}_label_{X}.csv\")\n",
    "\n",
    "## Unweighted dataset (2 A, 1 B)\n",
    "# Create an unweighted dataset with a 2:1 ratio of annotator types A to B\n",
    "\n",
    "# Filter the dataset to get labels from annotator type A and B separately\n",
    "df_a_biased = df_train[df_train['attribute'] == 0]  # Labels from annotator type A\n",
    "df_b_biased = df_train[df_train['attribute'] == 1]  # Labels from annotator type B\n",
    "\n",
    "# Randomly sample half of the B-biased labels\n",
    "half_length = len(df_b_biased) // 2\n",
    "rows_to_drop = df_b_biased.sample(n=half_length).index\n",
    "df_reduced = df_b_biased.drop(rows_to_drop)\n",
    "\n",
    "# Combine the full A-biased dataset with the reduced B-biased dataset\n",
    "df_unweighted = pd.concat([df_a_biased, df_reduced], axis=0)\n",
    "df_unweighted.to_csv(f\"data/unweighted_{L}_label_{X}.csv\")\n",
    "\n",
    "## Weighted dataset (2 A, 1 B * 2)\n",
    "\n",
    "# Create a weighted dataset where the B labels are duplicated to balance with A labels\n",
    "\n",
    "# Duplicate each row in the reduced B dataset to balance the count with A labels\n",
    "df_duplicated = df_reduced.loc[df_reduced.index.repeat(2)].reset_index(drop=True)\n",
    "\n",
    "# Combine the full A-biased dataset with the duplicated B-biased dataset\n",
    "df_weighted = pd.concat([df_a_biased, df_duplicated], axis=0)\n",
    "\n",
    "df_weighted.to_csv(f\"data/weighted_{L}_label_{X}.csv\")\n",
    "\n",
    "## Additional Unweighted dataset (3 A, 1 B)\n",
    "# Randomly sample half of the A-biased labels\n",
    "half_a_length = len(df_a_biased) // 2\n",
    "df_a_sampled = df_a_biased.sample(n=half_a_length)\n",
    "\n",
    "# Double the sampled A-biased labels to make 3 A's\n",
    "df_a_tripled = pd.concat([df_a_biased, df_a_sampled]).reset_index(drop=True)\n",
    "\n",
    "# Combine the tripled A-biased dataset with the full B-biased dataset\n",
    "df_unweighted_3A_1B = pd.concat([df_a_tripled, df_reduced], axis=0)\n",
    "\n",
    "df_unweighted_3A_1B.to_csv(f\"data/unweighted_3A1B_{L}_label_{X}.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
